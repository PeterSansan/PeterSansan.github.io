<!DOCTYPE html>
<html>
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  
  <title>blog of PeterSansan</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Focus on NLP">
<meta property="og:type" content="website">
<meta property="og:title" content="blog of PeterSansan">
<meta property="og:url" content="http://petersansan.top/page/4/index.html">
<meta property="og:site_name" content="blog of PeterSansan">
<meta property="og:description" content="Focus on NLP">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="blog of PeterSansan">
<meta name="twitter:description" content="Focus on NLP">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  

</head>

<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">blog of PeterSansan</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">Just Do It.</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
          <a class="main-nav-link" href="/about">About</a>
        
          <a class="main-nav-link" href="/other">Codes</a>
        
      </nav>
      <nav id="sub-nav">
        
        <a id="nav-search-btn" class="nav-icon" title="搜索"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://petersansan.top"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-《facebook效应》读后感" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/05/04/《facebook效应》读后感/" class="article-date">
  <time datetime="2017-05-04T15:34:09.000Z" itemprop="datePublished">2017-05-04</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/读后感/">读后感</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/05/04/《facebook效应》读后感/">《facebook效应》读后感</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <a id="more"></a>
<p>  今天距我读完这本书估计有一个多月吧，现在才写读后感可能有些记忆模糊。在我读完这本书的在没看这本书之前，我一直觉得这本书可能会难懂，但读完这本书后，我觉得facebook的成长是有血有肉的，它就像是一个婴儿一样成长，而共同孕育它的伟大人物们，如扎克伯格也是在这过程中不断地成长，最后才能在每种大变动中克服各种困难，最终使得facebook成长为一个巨人。</p>
<p>  许多人也说了，facebook是建立在与中国不同国情与社会背景建立起来的，按中国现在类似的软件可能是人人网，qq空间，微信朋友圈。但随着twitter化之后，也有些类似于微博。facebook的特点是把实际生活中的人类关系搬到了互联网上，且依靠互联网的作用把社交范围最大安全的扩大。因为facebook是采用实名制的，所以相对来说交友是比较安全，交流环境也是比较真实有效的。相比来说，较早一代的社交网站还是采用为昵称为网名的方式，虽然这种网站目前都会采用自愿实名的方式，但是人们在选择与接受中是依靠不同的文化背景的。相对于欧美，中国人整体对隐私的保护更为重视，虽然在主动有效保护的措施的意识还是不强。所以像腾讯这种社交模式不可能像facebook一样，一个伟人就要有很多好友，它重在你的朋友，你认识的人。而微博这种虽有大量的粉丝，但好朋友的频率交流不会用到这个，它更像是一个展示平台。有人说facebook和twitter是国内社交的学习模板，是有一样道理的。但之所以facebook不会在中国兴起的原因，还是与中国的人群特点有一到的关系。</p>
<p>  facebook的建立过程让我印象深刻，每一次扩大用户，每一次的融资，每一次的产品更新，都会引起 用户的最大关注与反响。相对于产品，我更加关注比我大不了多少的扎克伯格。这本书真实的揭露了扎克伯格在面纱。他一开始进入大学时，是一个编程天才，就算是是个天才，但在哈佛这种人才济济的地方，也不算太出奇。当然对于中国人来说，中国对孩子在科学技术技能的先天培养上真的差太多了。扎克伯格与其他优秀学生一样，在进入大学会就与志同道合的人一起编写各种应用。人与人的交往合作真的是需要有认同感，并不是所有能力出众的人都能够愿意一起合作。像扎克伯格早期与一双皮艇运动员的程序员合作，结果观点不同不了了之。最后随着facebook的强大，而上演官司大剧。扎克伯格一开始也不是一个合理格的公司CEO，他早期遇到了一个贵人，一个曾经在自己创立公司吃过大亏的彼得。他的经验帮助了一帮优秀的程序员度过刚创公司早期的困境。也给了马克成长的时间与空间。他力保马克那边占有三个董事三个席位的做法让人印象深刻。当然马克也是一个虚心的人，他会像他觉得值得尊敬的长辈学习。</p>
<p>  马克最在的特点是他对于产品的执着，他不像其他投资人看重的是公司的利润，他看重的是产品本身的重用与价值。在facebook发展的好长一段时间内，马克都反对引进大量的传统广告，使得facebook只能维持在公司正常运作的边缘上，谈不上有什么收入。马克他有它自己的想法，他有打造一个真正有意义的产品，可以最大方便人们分享与交流的产品，传统广告的加入会破坏产品的体验。最后直到有更贴切的广告设计出来，facebook用真实走上了商业之路。但这时的facebook已慢慢显现巨人的潜力了。马克虽然出身于程序员，对商业的管理虽还比较生疏，但对于产品的设计理念有自己理想的一套。这让我们看到了梦想的力量，facebook就是扎克伯格的梦想，而不只是一个赚钱的工具。</p>
<p>  我们也有梦想，每个人都有梦想，有些人会想的大些，有些人会想的小些，但能够坚持去做，并做好的并不多。这种人其实已经算是成功了，当然是做建立在美好梦想的前提上。我也有梦想，我的大梦已经破碎了，现在只有一个个小小的梦，如果能够实现这样一个个小小的梦，我也是满意我的一生。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://petersansan.top/2017/05/04/《facebook效应》读后感/" data-id="cj7ix9s7e000odrif8j4kguca" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/读后感/">读后感</a></li></ul>

    </footer>
	
  </div>
  
</article>



  
    <article id="post-《再穷也要去旅行》读后感" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/05/04/《再穷也要去旅行》读后感/" class="article-date">
  <time datetime="2017-05-04T15:31:40.000Z" itemprop="datePublished">2017-05-04</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/读后感/">读后感</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/05/04/《再穷也要去旅行》读后感/">《再穷也要去旅行》读后感</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <a id="more"></a>
<p>  我读这本书是在研二开学不久的一段时间。上一本书我读了《平凡的世界》，这是一本文学书，与《再穷也要去旅行》这本随笔的书籍在手法上相差很大。因此当我第一次看这本书时，总觉得这本书的文章的文笔很一般，就像是日记体的，当然这与作者不是一个专业的作家有关。当然我也不会太多关注书中的文笔，重点还是里面的故事与及作者的感悟。</p>
<p>  虽然我很少旅游，不能算爱旅游，但毕竟与作者是同龄人，在感情有会有一些共鸣。作者是一个马来西亚人，曾在中国留学过。我具体已不记得每个风景的细节，只记得她大概是从韩国开始，再从中国南京出发一直到西藏，尼泊尔等地方。在世界各地，与作者一样的背包客并不少，在中国也是，大部分旅客还是很省的，作者是个学生，更是如此，去西藏竟然是坐黑车“偷偷”过去的，还好司机都是好人，算是有惊无险。每每读到这些，心里总会有一些波动，一个女孩子竟会如此的勇敢，但又会觉得有些莽撞。在勇敢与莽撞间，我不知要怎样去做出更好的决定？作者到了高原后，不意间得了高原病，情况十分严重，不过她竟然不顾病大玩，还又出行游玩了一趟，真是冒险。虽然看到了美丽的风景，不过也付出了巨大的代价。</p>
<p>  之后，她去了东南亚，记得在吃东西很便宜的泰国，作者把她爱吃烤肉与冰淇淋的吃货特色反挥得淋漓尽致，尤其是冰淇淋。当然还少不了泰国的人妖啦，我也算是第一次从文章中知道人妖是怎样的。印度，不是一个好地方，印度租房贵，污染又十分严重，印度会还耍小伎俩，坑游客的钱。印度总问中国为什么不会印度旅游，大概都吓怕了，换成我，我与不会去。</p>
<p>  她之后去了欧洲，欧洲虽然不算大，但每一个国家都有一个国家的特点，像英国消费很贵，法国的优雅（作者眼里尽是忧伤）等。我不记得所有作者去过的欧洲地方，我也不想再去翻书查证。在欧洲的旅游算是比较幸运的，她在各个国家都有一些朋友或是好心人，有些会给她提供住所，有些给她提供打工的地方。在英国，只记得各种公园和各种贵，这让我想给了中国的北京，哈哈。法国，她想起了他的前男友。女生真是一种奇怪的人，至少对于男生来说是这样的，她也没有告诉我们她为什么不想与她做朋友了，但后面却想去看联系他，真是够了。我这段时间也与一个女生弄的没有一个好下场，读到这里，我算是释怀了，每个人都是独立的个体，爱与被爱真没有标准，大概就是不是正确的时机，没有遇到对的人吧。后来，她也会遇到很优秀的人，但都不敢再去爱了。分手对于双方来说都是一种伤害。</p>
<p>  作者的勇敢还表现在荷兰的阿姆斯特丹，那是个妓女与毒品合法的地方，她也去转了几趟，只是想看看有没有亚洲面孔的性工作者。旅行者出门在外的确如手足一般，这要是同行的，都会相互关心帮助，作者在这段时间旅行遇到了不少韩国人，她也特别喜欢吃韩国的泡菜。在欧洲也过了个不错的圣诞节，不过也遇到了小车祸，和为了等火车，而在电梯里面过夜的情况。</p>
<p>  除了与她男朋友的情感问题外，书中还有一个场景让我共鸣很大。作者在欧洲的某一段时间申请到了苏格兰的一个小岛上打工的机会，本来那是一个修心养性的一个好地方，却被她妈妈的死讯打破了。人的生命真得十分脆弱，一不留神就会消失，什么功名利实禄，都会灰飞烟灭，只剩下爱她的那些人在时间的车轮中舔着伤口，直到不再悲伤。作者整整哭了三天，她也无法立马回家，想想今年我去世的亲人，我感同身受，我不只一次的伤心自责堕落过，几度失去了前进的方面，麻木到只要活着就行了，但是现在我发现即使这样也不济于事。我虽然不是家人52年时光的全部，但也是一份爱的结晶，我要勇敢地活着，过着他们寄于我的精彩的人生。我曾是他的骄傲，过去是，将来也是。</p>
<p>  这本书内容很少，她是一个涉世不深的姑娘的所见所闻，但依旧值得我去读它。她帮我分担了悲伤，帮我看到了希望，让我看到了勇敢与坚强，正如上一本书提到的一样，我们生来平凡，但我们每一个人的人生都不平凡，这众多不平凡其实也是一种平凡，做是却是真真正正的每一个，每一段经历。致敬我的青春，致敬我的世界。虽然我不怎样会爱旅行，但如果有机会我还是会与自己喜欢的朋友，家人一起去旅行，说不定这真的会打开我另一扇窗。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://petersansan.top/2017/05/04/《再穷也要去旅行》读后感/" data-id="cj7ix9s7i000udrifmk84wf63" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/读后感/">读后感</a></li></ul>

    </footer>
	
  </div>
  
</article>



  
    <article id="post-《三体》读后感" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/05/04/《三体》读后感/" class="article-date">
  <time datetime="2017-05-04T15:31:17.000Z" itemprop="datePublished">2017-05-04</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/读后感/">读后感</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/05/04/《三体》读后感/">《三体》读后感</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
         <a id="more"></a>
<p>  我大概用了两周的时间读完了这本书，《三体》的这种硬科幻的小说，现实感非常强，读到后面一度有种感觉，这就是我们宇宙的命运。里面的宇宙理论不知是否来自我们世界的研究，但在小说中的确无懈可击，让我对宇宙充满了一定的认知，虽然这个认知是否完全符合我们的世界，我也不会过多地追究</p>
<p>  世界上没有两片完全相同的叶子，每个人的遭遇和个性是不同的，这导致每个人的思想都不分歧。叶文洁是第一个希望外来文明改造地球的人，她发出的信号被三体星球收到。之后三体势力的动作，直接威胁到了地球的安全。地球的弱势是人类对宇宙规律的无知，像黑暗森林理论，低维攻击等等。总之，还是技术与理论的落后导致了这场浩劫，不过地球只是无数低级文明的一个代表，最后整个宇宙都会遭到同一个命运，即被不同的文明所消灭，最后一起走向灭亡，这就是宇宙的命运。第一部的汪淼，第二部的罗辑，第三部的程心，都作为地球中最杰出的代表对三体势力做着对抗，想保护地球免受破坏，却不知后面还有更大的危机。小说中的时间跨度很大，这才完整描述宇宙的整个过程，如果只是一个不冬眠普通人的视野，只会是一个人的一生，而不是宇宙的一生，程心是幸运的，她的一生的终点就是宇宙的末日。</p>
<p>  汪淼本是一个普通的研究纳米技术的科学家，以亲身经历得出了三体与智子的存在，是地球危机的发现者。罗辑本是一个半调子的博士生，意外成了三体追杀的对象，因此成了面壁者。他是一个伟大的面壁者和执剑人，是他给了地球希望和在一段时间保护了地球，但人毕竟不是上帝，不能一直战斗中，他最后还是变成了一种图腾和引路人，为地球的革命军和后面的程心等奋战的人给予帮助。程心是一个女性，她有善良的心，有大爱之情，但也有柔弱的一面，他没有像罗辑一样代现出来的神奇的作用，她更像是一个人类对宇宙演变过程的见证者，她作为执剑人似乎很失败，但就算是其他人作了执剑人，地球的命运也会是如此。</p>
<p>  三体小说唯一让人遗憾的是结尾，原著中的结尾的确没有交待清楚，好在从网上看到了有人把原作者的真正的结尾公布出来，但毕竟缺少了结尾文字上的描写，少了一定的完整性。确实有些美中不足。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://petersansan.top/2017/05/04/《三体》读后感/" data-id="cj7ix9s7g000sdrifjyxhlgy7" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/读后感/">读后感</a></li></ul>

    </footer>
	
  </div>
  
</article>



  
    <article id="post-采样采样" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/04/29/采样采样/" class="article-date">
  <time datetime="2017-04-29T07:11:57.000Z" itemprop="datePublished">2017-04-29</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/理论/">理论</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/04/29/采样采样/">几种采样区分</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p><img src="http://ogtxggxo6.bkt.clouddn.com/fdf.png?imageslim" alt=""></p>
        
          <p class="article-more-link">
            <a href="/2017/04/29/采样采样/#more">Read More</a>
          </p>
        
      
    </div>
    <footer class="article-footer">
      <a data-url="http://petersansan.top/2017/04/29/采样采样/" data-id="cj7ix9s9c0056drif6aybtav8" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/理论/">理论</a></li></ul>

    </footer>
	
  </div>
  
</article>



  
    <article id="post-转-理解LSTM网络" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/04/29/转-理解LSTM网络/" class="article-date">
  <time datetime="2017-04-28T16:49:29.000Z" itemprop="datePublished">2017-04-29</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/深度学习/">深度学习</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/04/29/转-理解LSTM网络/">转:理解LSTM网络</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <a id="more"></a>
<p>声明：本文转自<a href="http://www.jianshu.com/p/9dc9f41f0b29" target="_blank" rel="external">简书</a></p>
<h2 id="1、RNN来啦"><a href="#1、RNN来啦" class="headerlink" title="1、RNN来啦"></a>1、RNN来啦</h2><p>人类并不是每时每刻都从一片空白的大脑开始他们的思考。在你阅读这篇文章时候，你都是基于自己已经拥有的对先前所见词的理解来推断当前词的真实含义。我们不会将所有的东西都全部丢弃，然后用空白的大脑进行思考。我们的思想拥有持久性。<br>传统的神经网络并不能做到这点，看起来也像是一种巨大的弊端。例如，假设你希望对电影中的每个时间点的时间类型进行分类。传统的神经网络应该很难来处理这个问题——使用电影中先前的事件推断后续的事件。<br><code>RNN</code> 解决了这个问题。<code>RNN</code> 是包含循环的网络，允许信息的持久化。</p>
<p><img src="http://ogtxggxo6.bkt.clouddn.com/rnn1.png?imageView2/2/w/200" alt="RNN"></p>
<p>在上面的示例图中，神经网络的模块<code>A</code>，正在读取某个输入 $x_i$，并输出一个值$h_i$。循环可以使得信息可以从当前步传递到下一步。<br>这些循环使得 <code>RNN</code>看起来非常神秘。然而，如果你仔细想想，这样也不比一个正常的神经网络难于理解。<code>RNN</code> 可以被看做是同一神经网络的多次复制，每个神经网络模块会把消息传递给下一个。所以，如果我们将这个循环展开：</p>
<p><img src="http://ogtxggxo6.bkt.clouddn.com/rnn2.png?imageView2/2/w/650" alt="展开的RNN"></p>
<p>链式的特征揭示了<code>RNN</code>本质上是与序列和列表相关的。他们是对于这类数据的最自然的神经网络架构。<br>并且 RNN 也已经被人们应用了！在过去几年中，应用<code>RNN</code>在语音识别，语言建模，翻译，图片描述等问题上已经取得一定成功，并且这个列表还在增长。我建议大家参考 <code>Andrej Karpathy</code> 的博客文章——<a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" target="_blank" rel="external">The Unreasonable Effectiveness of Recurrent Neural Networks</a> 来看看更丰富有趣的<code>RNN</code>的成功应用。<br>而这些成功应用的关键之处就是<code>LSTM</code>的使用，这是一种特别的<code>RNN</code>，比标准的<code>RNN</code>在很多的任务上都表现得更好。几乎所有的令人振奋的关于<code>RNN</code>的结果都是通过 <code>LSTM</code>达到的。这篇博文也会就 <code>LSTM</code>进行展开。</p>
<h2 id="2、长期依赖（Long-Term-Dependencies）问题"><a href="#2、长期依赖（Long-Term-Dependencies）问题" class="headerlink" title="2、长期依赖（Long-Term Dependencies）问题"></a>2、长期依赖（Long-Term Dependencies）问题</h2><p><code>RNN</code>的关键点之一就是他们可以用来连接先前的信息到当前的任务上，例如使用过去的视频段来推测对当前段的理解。如果 <code>RNN</code>可以做到这个，他们就变得非常有用。但是真的可以么？答案是，还有很多依赖因素。<br>有时候，我们仅仅需要知道先前的信息来执行当前的任务。例如，我们有一个语言模型用来基于先前的词来预测下一个词。如果我们试着预测 <code>“the clouds are in the sky”</code> 最后的词，我们并不需要任何其他的上下文 —— 因此下一个词很显然就应该是 <code>sky</code>。在这样的场景中，相关的信息和预测的词位置之间的间隔是非常小的，<code>RNN</code> 可以学会使用先前的信息。</p>
<p><img src="http://ogtxggxo6.bkt.clouddn.com/rnn4.png?imageView2/2/w/650" alt="不太长的相关信息和位置间隔"></p>
<p>但是同样会有一些更加复杂的场景。假设我们试着去预测<code>“I grew up in France... I speak fluent French”</code>最后的词。当前的信息建议下一个词可能是一种语言的名字，但是如果我们需要弄清楚是什么语言，我们是需要先前提到的离当前位置很远的 <code>France</code>的上下文的。这说明相关信息和当前预测位置之间的间隔就肯定变得相当的大。<br>不幸的是，在这个间隔不断增大时，<code>RNN</code>会丧失学习到连接如此远的信息的能力。</p>
<p><img src="http://ogtxggxo6.bkt.clouddn.com/rnn5.png?imageView2/2/w/650" alt="相当长的相关信息和位置间隔"></p>
<p>在理论上，RNN 绝对可以处理这样的 长期依赖 问题。人们可以仔细挑选参数来解决这类问题中的最初级形式，但在实践中，RNN 肯定不能够成功学习到这些知识。<a href="http://www-dsi.ing.unifi.it/~paolo/ps/tnn-94-gradient.pdf" target="_blank" rel="external">Bengio, et al. (1994)</a>等人对该问题进行了深入的研究，他们发现一些使训练 RNN 变得非常困难的相当根本的原因。<br>然而，幸运的是，LSTM 并没有这个问题！</p>
<h2 id="3、LSTM-网络"><a href="#3、LSTM-网络" class="headerlink" title="3、LSTM 网络"></a>3、LSTM 网络</h2><p><code>Long Short Term</code>网络—— 一般就叫做 <code>LSTM</code> ——是一种 <code>RNN</code> 特殊的类型，可以学习长期依赖信息。LSTM 由<a href="http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf" target="_blank" rel="external">Hochreiter &amp; Schmidhuber (1997)</a>提出，并在近期被<a href="https://scholar.google.com/citations?user=DaFHynwAAAAJ&amp;hl=en" target="_blank" rel="external">Alex Graves</a>进行了改良和推广。在很多问题，<code>LSTM</code> 都取得相当巨大的成功，并得到了广泛的使用。<br><code>LSTM</code>通过刻意的设计来避免长期依赖问题。记住长期的信息在实践中是 <code>LSTM</code> 的默认行为，而非需要付出很大代价才能获得的能力！<br>所有 <code>RNN</code> 都具有一种重复神经网络模块的链式的形式。在标准的 <code>RNN</code> 中，这个重复的模块只有一个非常简单的结构，例如一个 <code>tanh</code> 层。</p>
<p><img src="http://ogtxggxo6.bkt.clouddn.com/rnn6.png?imageView2/2/w/650" alt="标准 RNN 中的重复模块包含单一的层"></p>
<p><code>LSTM</code> 同样是这样的结构，但是重复的模块拥有一个不同的结构。不同于 单一神经网络层，这里是有四个，以一种非常特殊的方式进行交互。</p>
<p><img src="http://ogtxggxo6.bkt.clouddn.com/rnn7.png?imageView2/2/w/650" alt="LSTM 中的重复模块包含四个交互的层"></p>
<p>不必担心这里的细节。我们会一步一步地剖析 LSTM 解析图。现在，我们先来熟悉一下图中使用的各种元素的图标。</p>
<p><img src="http://ogtxggxo6.bkt.clouddn.com/rnn8.png?imageView2/2/w/650" alt="LSTM 中的图标"></p>
<p>在上面的图例中，每一条黑线传输着一整个向量，从一个节点的输出到其他节点的输入。粉色的圈代表<code>pointwise</code>的操作，诸如向量的和，而黄色的矩阵就是学习到的神经网络层。合在一起的线表示向量的连接，分开的线表示内容被复制，然后分发到不同的位置。</p>
<h2 id="4、LSTM-的核心思想"><a href="#4、LSTM-的核心思想" class="headerlink" title="4、LSTM 的核心思想"></a>4、LSTM 的核心思想</h2><p><code>LSTMs</code>的关键点是单元状态，就是穿过图中的水平线。单元状态有点像是个传送带。它贯穿整个链条，只有一些小的线性相互作用。这很容易让信息以不变的方式向下流动。</p>
<p><img src="http://ogtxggxo6.bkt.clouddn.com/rnn9.png?imageView2/2/w/650" alt=""></p>
<p><code>LSTM</code> 有通过精心设计的称作为“门”的结构来去除或者增加信息到细胞状态的能力。门是一种让信息选择式通过的方法。他们包含一个 <code>sigmoid</code> 神经网络层和一个 <code>pointwise</code> 逐点乘法操作。</p>
<p><img src="http://ogtxggxo6.bkt.clouddn.com/rnn10.png?imageView2/2/w/650" alt=""></p>
<p><code>Sigmoid</code>层输出 0 到 1 之间的数值，描述每个部分有多少量可以通过。0 代表“不许任何量通过”，1 就指“允许任意量通过”！</p>
<p><code>LSTM</code>拥有三个门，来保护和控制细胞状态。</p>
<h2 id="4、逐步理解-LSTM"><a href="#4、逐步理解-LSTM" class="headerlink" title="4、逐步理解 LSTM"></a>4、逐步理解 LSTM</h2><p>在我们<code>LSTM</code>中的第一步是决定我们会从细胞状态中丢弃什么信息。这个决定通过一个称为<strong>遗忘门</strong>完成。该门会读取 $h_{t-1}$ 和 $x<em>t$，输出一个在 0 到 1 之间的数值给每个在细胞状态 $C</em>{t-1}$ 中的数字。1 表示“完全保留”，0 表示“完全舍弃”。<br>让我们回到语言模型的例子中来基于已经看到的预测下一个词。在这个问题中，细胞状态可能包含当前<strong>主语</strong>的性别，因此正确的<strong>代词</strong>可以被选择出来。当我们看到新的<strong>主语</strong>，我们希望忘记旧的<strong>主语</strong>。</p>
<p><img src="http://ogtxggxo6.bkt.clouddn.com/rnn11.png?imageView2/2/w/650" alt=""></p>
<p>下一步是确定什么样的新信息被存放在细胞状态中。这里包含两个部分。第一，<code>sigmoid</code>层称 “<strong>输入门</strong>” 决定什么值我们将要更新。然后，一个 <code>tanh</code> 层创建一个新的候选值向量$\tilde{C}_t$，会被加入到状态中。下一步，我们会讲这两个信息来产生对状态的更新。<br>在我们语言模型的例子中，我们希望增加新的主语的性别到细胞状态中，来替代旧的需要忘记的主语。</p>
<p><img src="http://ogtxggxo6.bkt.clouddn.com/rnn12.png?imageView2/2/w/650" alt=""></p>
<p>现在是更新旧细胞状态的时间了，$C_{t-1}$ 更新为 $C_t$。前面的步骤已经决定了将会做什么，我们现在就是实际去完成。<br>我们把旧状态与 $f_t$ 相乘，丢弃掉我们确定需要丢弃的信息。接着加上 $i_t * \tilde{C}_t$。这就是新的候选值，根据我们决定更新每个状态的程度进行变化。<br>在语言模型的例子中，这就是我们实际根据前面确定的目标，丢弃旧代词的性别信息并添加新的信息的地方。</p>
<p><img src="http://ogtxggxo6.bkt.clouddn.com/rnn13.png?imageView2/2/w/650" alt=""></p>
<p>最终，我们需要确定输出什么值，即“<strong>输出门</strong>”。这个输出将会基于我们的细胞状态，但是也是一个过滤后的版本。首先，我们运行一个<code>sigmoid</code> 层来确定细胞状态的哪个部分将输出出去。接着，我们把细胞状态通过<code>tanh</code>进行处理（得到一个在 -1 到 1 之间的值）并将它和 <code>sigmoid</code> 门的输出相乘，最终我们仅仅会输出我们确定输出的那部分。<br>在语言模型的例子中，因为他就看到了一个 <strong>代词</strong>，可能需要输出与一个 <strong>动词</strong> 相关的信息。例如，可能输出是否代词是单数还是负数，这样如果是动词的话，我们也知道动词需要进行的词形变化。</p>
<p><img src="http://ogtxggxo6.bkt.clouddn.com/rnn14.png?imageView2/2/w/650" alt=""></p>
<h2 id="5、LSTM-的变体"><a href="#5、LSTM-的变体" class="headerlink" title="5、LSTM 的变体"></a>5、LSTM 的变体</h2><p>我们到目前为止都还在介绍正常的 LSTM。但是不是所有的 LSTM 都长成一个样子的。实际上，几乎所有包含 LSTM 的论文都采用了微小的变体。差异非常小，但是也值得拿出来讲一下。<br>其中一个流形的 LSTM 变体，就是由 <a href="ftp://ftp.idsia.ch/pub/juergen/TimeCount-IJCNN2000.pdf" target="_blank" rel="external">Gers &amp; Schmidhuber (2000)</a> 提出的，增加了 “peephole connection”。是说，我们让 门层 也会接受细胞状态的输入。</p>
<p><img src="http://ogtxggxo6.bkt.clouddn.com/rnn15.png?imageView2/2/w/650" alt=""></p>
<p>上面的图例中，我们增加了<code>peephole</code> 到每个门上，但是许多论文会加入部分的 <code>peephole</code> 而非所有都加。</p>
<p>另一个变体是通过使用 <code>coupled</code>耦合忘记和输入门。不同于之前是分开确定什么忘记和需要添加什么新的信息，这里是一同做出决定。我们仅仅会当我们将要输入在当前位置时忘记。我们仅仅输入新的值到那些我们已经忘记旧的信息的那些状态 。</p>
<p><img src="http://ogtxggxo6.bkt.clouddn.com/rnn16.png?imageView2/2/w/650" alt=""></p>
<p>另一个改动较大的变体是 Gated Recurrent Unit (GRU)，这是由 <a href="http://arxiv.org/pdf/1406.1078v3.pdf" target="_blank" rel="external">Cho, et al. (2014)</a> 提出。它将忘记门和输入门合成了一个单一的 更新门。同样还混合了细胞状态和隐藏状态，和其他一些改动。最终的模型比标准的 LSTM 模型要简单，也是非常流行的变体。</p>
<p><img src="http://ogtxggxo6.bkt.clouddn.com/rnn17.png?imageView2/2/w/650" alt=""></p>
<p>这里只是部分流行的 <code>LSTM</code> 变体。当然还有很多其他的，如<a href="http://arxiv.org/pdf/1508.03790v2.pdf" target="_blank" rel="external">Yao, et al. (2015)</a> 提出的 <code>Depth Gated RNN</code>。还有用一些完全不同的观点来解决长期依赖的问题，如<a href="http://arxiv.org/pdf/1402.3511v1.pdf" target="_blank" rel="external">Koutnik, et al. (2014)</a> 提出的 <code>Clockwork RNN</code>。<br>要问哪个变体是最好的？其中的差异性真的重要吗？<a href="http://arxiv.org/pdf/1503.04069.pdf" target="_blank" rel="external">Greff, et al. (2015)</a> 给出了流行变体的比较，结论是他们基本上是一样的。<a href="http://jmlr.org/proceedings/papers/v37/jozefowicz15.pdf" target="_blank" rel="external">Jozefowicz, et al. (2015)</a> 则在超过 1 万种 <code>RNN</code> 架构上进行了测试，发现一些架构在某些任务上也取得了比<code>LSTM</code>更好的结果。</p>
<p><img src="http://ogtxggxo6.bkt.clouddn.com/rnn18.png?imageView2/2/w/650" alt=""></p>
<h2 id="八、结论"><a href="#八、结论" class="headerlink" title="八、结论"></a>八、结论</h2><p>刚开始，我提到通过 <code>RNN</code> 得到重要的结果。本质上所有这些都可以使用 <code>LSTM</code> 完成。对于大多数任务确实展示了更好的性能！<br>由于<code>LSTM</code>一般是通过一系列的方程表示的，使得 <code>LSTM</code>有一点令人费解。然而本文中一步一步地解释让这种困惑消除了不少。<br><code>LSTM</code>是我们在<code>RNN</code>中获得的重要成功。很自然地，我们也会考虑：哪里会有更加重大的突破呢？在研究人员间普遍的观点是：“Yes! 下一步已经有了——那就是<strong>注意力</strong>！” 这个想法是让 <code>RNN</code> 的每一步都从更加大的信息集中挑选信息。例如，如果你使用 RNN 来产生一个图片的描述，可能会选择图片的一个部分，根据这部分信息来产生输出的词。实际上，<a href="http://arxiv.org/pdf/1502.03044v2.pdf" target="_blank" rel="external">Xu, <em>et al.</em>(2015)</a>已经这么做了——如果你希望深入探索<strong>注意力</strong>可能这就是一个有趣的起点！还有一些使用注意力的相当振奋人心的研究成果，看起来有更多的东西亟待探索……<br>注意力也不是 RNN 研究领域中唯一的发展方向。例如，<a href="http://arxiv.org/pdf/1507.01526v1.pdf" target="_blank" rel="external">Kalchbrenner, <em>et al.</em> (2015)</a> 提出的 <code>Grid LSTM</code>看起来也是很有前途。使用生成模型的 <code>RNN</code>，诸如<a href="http://arxiv.org/pdf/1502.04623.pdf" target="_blank" rel="external">Gregor, <em>et al.</em> (2015)</a> <a href="http://arxiv.org/pdf/1506.02216v3.pdf" target="_blank" rel="external">Chung, <em>et al.</em> (2015)</a> 和 <a href="http://arxiv.org/pdf/1411.7610v3.pdf" target="_blank" rel="external">Bayer &amp; Osendorfer (2015)</a> 提出的模型同样很有趣。在过去几年中，<code>RNN</code> 的研究已经相当的燃，而研究成果当然也会更加丰富！</p>
<h2 id="七、致谢"><a href="#七、致谢" class="headerlink" title="七、致谢"></a>七、致谢</h2><p>I’m grateful to a number of people for helping me better understand LSTMs, commenting on the visualizations, and providing feedback on this post.<br>I’m very grateful to my colleagues at Google for their helpful feedback, especially <a href="http://research.google.com/pubs/OriolVinyals.html" target="_blank" rel="external">Oriol Vinyals</a>,<a href="http://research.google.com/pubs/GregCorrado.html" target="_blank" rel="external">Greg Corrado</a>, <a href="http://research.google.com/pubs/JonathonShlens.html" target="_blank" rel="external">Jon Shlens</a>, <a href="http://people.cs.umass.edu/~luke/" target="_blank" rel="external">Luke Vilnis</a>, and <a href="http://www.cs.toronto.edu/~ilya/" target="_blank" rel="external">Ilya Sutskever</a>. I’m also thankful to many other friends and colleagues for taking the time to help me, including <a href="https://www.linkedin.com/pub/dario-amodei/4/493/393" target="_blank" rel="external">Dario Amodei</a>, and <a href="http://cs.stanford.edu/~jsteinhardt/" target="_blank" rel="external">Jacob Steinhardt</a>. I’m especially thankful to <a href="http://www.kyunghyuncho.me/" target="_blank" rel="external">Kyunghyun Cho</a> for extremely thoughtful correspondence about my diagrams.<br>Before this post, I practiced explaining LSTMs during two seminar series I taught on neural networks. Thanks to everyone who participated in those for their patience with me, and for their feedback.</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://petersansan.top/2017/04/29/转-理解LSTM网络/" data-id="cj7ix9s9b0054drifwwtg8kvk" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/lstm/">lstm</a></li></ul>

    </footer>
	
  </div>
  
</article>



  
    <article id="post-论文记录0" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/04/28/论文记录0/" class="article-date">
  <time datetime="2017-04-28T15:51:06.000Z" itemprop="datePublished">2017-04-28</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/论文记录/">论文记录</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/04/28/论文记录0/">RECURRENT NEURAL NETWORK REGULARIZATION</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p><img src="http://ogtxggxo6.bkt.clouddn.com/rnn.jpg?imageslim" alt=""></p>
        
          <p class="article-more-link">
            <a href="/2017/04/28/论文记录0/#more">Read More</a>
          </p>
        
      
    </div>
    <footer class="article-footer">
      <a data-url="http://petersansan.top/2017/04/28/论文记录0/" data-id="cj7ix9s8j002rdrifjtpbpvgv" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/LSTM/">LSTM</a></li></ul>

    </footer>
	
  </div>
  
</article>



  


  <nav id="page-nav">
    <a class="extend prev" rel="prev" href="/page/3/">&laquo; __('prev')</a><a class="page-number" href="/">1</a><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><span class="page-number current">4</span><a class="page-number" href="/page/5/">5</a><a class="page-number" href="/page/6/">6</a><a class="page-number" href="/page/7/">7</a><a class="extend next" rel="next" href="/page/5/">__('next') &raquo;</a>
  </nav>
</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">分类</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Keras/">Keras</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/NLP/">NLP</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/tensorflow/">tensorflow</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/工具/">工具</a><span class="category-list-count">6</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/序列标注/">序列标注</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/服务器/">服务器</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/机器学习/">机器学习</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/深度学习/">深度学习</a><span class="category-list-count">6</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/理论/">理论</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/算法/">算法</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/论文记录/">论文记录</a><span class="category-list-count">6</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/读后感/">读后感</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/资源/">资源</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/随笔/">随笔</a><span class="category-list-count">3</span></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">标签云</h3>
    <div class="widget tagcloud">
      <a href="/tags/Keras/" style="font-size: 10px;">Keras</a> <a href="/tags/LSTM/" style="font-size: 13.33px;">LSTM</a> <a href="/tags/NLP/" style="font-size: 16.67px;">NLP</a> <a href="/tags/github/" style="font-size: 13.33px;">github</a> <a href="/tags/hexo/" style="font-size: 10px;">hexo</a> <a href="/tags/jupyter/" style="font-size: 13.33px;">jupyter</a> <a href="/tags/lstm/" style="font-size: 10px;">lstm</a> <a href="/tags/nlp/" style="font-size: 10px;">nlp</a> <a href="/tags/tensorflow/" style="font-size: 10px;">tensorflow</a> <a href="/tags/ubuntu/" style="font-size: 10px;">ubuntu</a> <a href="/tags/webapp/" style="font-size: 10px;">webapp</a> <a href="/tags/中文分词/" style="font-size: 10px;">中文分词</a> <a href="/tags/价值观/" style="font-size: 10px;">价值观</a> <a href="/tags/工具/" style="font-size: 13.33px;">工具</a> <a href="/tags/序列标注/" style="font-size: 20px;">序列标注</a> <a href="/tags/服务器/" style="font-size: 10px;">服务器</a> <a href="/tags/机器学习/" style="font-size: 13.33px;">机器学习</a> <a href="/tags/概率图模型/" style="font-size: 10px;">概率图模型</a> <a href="/tags/深度学习/" style="font-size: 20px;">深度学习</a> <a href="/tags/理论/" style="font-size: 10px;">理论</a> <a href="/tags/算法/" style="font-size: 13.33px;">算法</a> <a href="/tags/综述/" style="font-size: 10px;">综述</a> <a href="/tags/读后感/" style="font-size: 16.67px;">读后感</a> <a href="/tags/转载/" style="font-size: 10px;">转载</a> <a href="/tags/递归/" style="font-size: 10px;">递归</a> <a href="/tags/随笔/" style="font-size: 10px;">随笔</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">标签</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Keras/">Keras</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/LSTM/">LSTM</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/NLP/">NLP</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/github/">github</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/hexo/">hexo</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/jupyter/">jupyter</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/lstm/">lstm</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/nlp/">nlp</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/tensorflow/">tensorflow</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ubuntu/">ubuntu</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/webapp/">webapp</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/中文分词/">中文分词</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/价值观/">价值观</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/工具/">工具</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/序列标注/">序列标注</a><span class="tag-list-count">7</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/服务器/">服务器</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/机器学习/">机器学习</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/概率图模型/">概率图模型</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/深度学习/">深度学习</a><span class="tag-list-count">7</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/理论/">理论</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/算法/">算法</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/综述/">综述</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/读后感/">读后感</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/转载/">转载</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/递归/">递归</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/随笔/">随笔</a><span class="tag-list-count">1</span></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">归档</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/09/">九月 2017</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/08/">八月 2017</a><span class="archive-list-count">8</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/05/">五月 2017</a><span class="archive-list-count">11</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/04/">四月 2017</a><span class="archive-list-count">19</span></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">最新文章</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2017/09/13/hello-world/">Hexo常用指令</a>
          </li>
        
          <li>
            <a href="/2017/09/13/test/">test</a>
          </li>
        
          <li>
            <a href="/2017/08/31/论文记录5/">A Long Dependency Aware Deep Architecture for Joint Chinese Word Segmentation and POS Tagging</a>
          </li>
        
          <li>
            <a href="/2017/08/31/论文记录4/">Attending to Characters in Neural Sequence Labeling Models</a>
          </li>
        
          <li>
            <a href="/2017/08/31/论文记录3/">End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2017 PeterSansan<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
    <a href="/about" class="mobile-nav-link">About</a>
  
    <a href="/other" class="mobile-nav-link">Codes</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>

  </div><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
</body>
</html>