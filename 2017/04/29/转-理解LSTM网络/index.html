<!DOCTYPE html>
<html>
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  
  <title>转:理解LSTM网络 | blog of PeterSansan</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="keywords" content="lstm">
<meta property="og:type" content="article">
<meta property="og:title" content="转:理解LSTM网络">
<meta property="og:url" content="http://petersansan.top/2017/04/29/转-理解LSTM网络/index.html">
<meta property="og:site_name" content="blog of PeterSansan">
<meta property="og:image" content="http://ogtxggxo6.bkt.clouddn.com/lstm2.png?imageslim">
<meta property="og:image" content="http://ogtxggxo6.bkt.clouddn.com/rnn1.png?imageView2/2/w/200">
<meta property="og:image" content="http://ogtxggxo6.bkt.clouddn.com/rnn2.png?imageView2/2/w/650">
<meta property="og:image" content="http://ogtxggxo6.bkt.clouddn.com/rnn4.png?imageView2/2/w/650">
<meta property="og:image" content="http://ogtxggxo6.bkt.clouddn.com/rnn5.png?imageView2/2/w/650">
<meta property="og:image" content="http://ogtxggxo6.bkt.clouddn.com/rnn6.png?imageView2/2/w/650">
<meta property="og:image" content="http://ogtxggxo6.bkt.clouddn.com/rnn7.png?imageView2/2/w/650">
<meta property="og:image" content="http://ogtxggxo6.bkt.clouddn.com/rnn8.png?imageView2/2/w/650">
<meta property="og:image" content="http://ogtxggxo6.bkt.clouddn.com/rnn9.png?imageView2/2/w/650">
<meta property="og:image" content="http://ogtxggxo6.bkt.clouddn.com/rnn10.png?imageView2/2/w/650">
<meta property="og:image" content="http://ogtxggxo6.bkt.clouddn.com/rnn11.png?imageView2/2/w/650">
<meta property="og:image" content="http://ogtxggxo6.bkt.clouddn.com/rnn12.png?imageView2/2/w/650">
<meta property="og:image" content="http://ogtxggxo6.bkt.clouddn.com/rnn13.png?imageView2/2/w/650">
<meta property="og:image" content="http://ogtxggxo6.bkt.clouddn.com/rnn14.png?imageView2/2/w/650">
<meta property="og:image" content="http://ogtxggxo6.bkt.clouddn.com/rnn15.png?imageView2/2/w/650">
<meta property="og:image" content="http://ogtxggxo6.bkt.clouddn.com/rnn16.png?imageView2/2/w/650">
<meta property="og:image" content="http://ogtxggxo6.bkt.clouddn.com/rnn17.png?imageView2/2/w/650">
<meta property="og:image" content="http://ogtxggxo6.bkt.clouddn.com/rnn18.png?imageView2/2/w/650">
<meta property="og:updated_time" content="2017-04-28T18:13:20.441Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="转:理解LSTM网络">
<meta name="twitter:image" content="http://ogtxggxo6.bkt.clouddn.com/lstm2.png?imageslim">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  

</head>

<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">blog of PeterSansan</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">Just Do It.</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
          <a class="main-nav-link" href="/about">About</a>
        
          <a class="main-nav-link" href="/other">TensorFlow</a>
        
      </nav>
      <nav id="sub-nav">
        
        <a id="nav-search-btn" class="nav-icon" title="搜索"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://petersansan.top"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-转-理解LSTM网络" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/04/29/转-理解LSTM网络/" class="article-date">
  <time datetime="2017-04-28T16:49:29.000Z" itemprop="datePublished">2017-04-29</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/深度学习/">深度学习</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      转:理解LSTM网络
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p><img src="http://ogtxggxo6.bkt.clouddn.com/lstm2.png?imageslim" alt=""></p>
<a id="more"></a>
<p>声明：本文转自<a href="http://www.jianshu.com/p/9dc9f41f0b29" target="_blank" rel="external">简书</a></p>
<h2 id="1、RNN来啦"><a href="#1、RNN来啦" class="headerlink" title="1、RNN来啦"></a>1、RNN来啦</h2><p>人类并不是每时每刻都从一片空白的大脑开始他们的思考。在你阅读这篇文章时候，你都是基于自己已经拥有的对先前所见词的理解来推断当前词的真实含义。我们不会将所有的东西都全部丢弃，然后用空白的大脑进行思考。我们的思想拥有持久性。<br>传统的神经网络并不能做到这点，看起来也像是一种巨大的弊端。例如，假设你希望对电影中的每个时间点的时间类型进行分类。传统的神经网络应该很难来处理这个问题——使用电影中先前的事件推断后续的事件。<br><code>RNN</code> 解决了这个问题。<code>RNN</code> 是包含循环的网络，允许信息的持久化。</p>
<p><img src="http://ogtxggxo6.bkt.clouddn.com/rnn1.png?imageView2/2/w/200" alt="RNN"></p>
<p>在上面的示例图中，神经网络的模块<code>A</code>，正在读取某个输入 $x_i$，并输出一个值$h_i$。循环可以使得信息可以从当前步传递到下一步。<br>这些循环使得 <code>RNN</code>看起来非常神秘。然而，如果你仔细想想，这样也不比一个正常的神经网络难于理解。<code>RNN</code> 可以被看做是同一神经网络的多次复制，每个神经网络模块会把消息传递给下一个。所以，如果我们将这个循环展开：</p>
<p><img src="http://ogtxggxo6.bkt.clouddn.com/rnn2.png?imageView2/2/w/650" alt="展开的RNN"></p>
<p>链式的特征揭示了<code>RNN</code>本质上是与序列和列表相关的。他们是对于这类数据的最自然的神经网络架构。<br>并且 RNN 也已经被人们应用了！在过去几年中，应用<code>RNN</code>在语音识别，语言建模，翻译，图片描述等问题上已经取得一定成功，并且这个列表还在增长。我建议大家参考 <code>Andrej Karpathy</code> 的博客文章——<a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" target="_blank" rel="external">The Unreasonable Effectiveness of Recurrent Neural Networks</a> 来看看更丰富有趣的<code>RNN</code>的成功应用。<br>而这些成功应用的关键之处就是<code>LSTM</code>的使用，这是一种特别的<code>RNN</code>，比标准的<code>RNN</code>在很多的任务上都表现得更好。几乎所有的令人振奋的关于<code>RNN</code>的结果都是通过 <code>LSTM</code>达到的。这篇博文也会就 <code>LSTM</code>进行展开。</p>
<h2 id="2、长期依赖（Long-Term-Dependencies）问题"><a href="#2、长期依赖（Long-Term-Dependencies）问题" class="headerlink" title="2、长期依赖（Long-Term Dependencies）问题"></a>2、长期依赖（Long-Term Dependencies）问题</h2><p><code>RNN</code>的关键点之一就是他们可以用来连接先前的信息到当前的任务上，例如使用过去的视频段来推测对当前段的理解。如果 <code>RNN</code>可以做到这个，他们就变得非常有用。但是真的可以么？答案是，还有很多依赖因素。<br>有时候，我们仅仅需要知道先前的信息来执行当前的任务。例如，我们有一个语言模型用来基于先前的词来预测下一个词。如果我们试着预测 <code>“the clouds are in the sky”</code> 最后的词，我们并不需要任何其他的上下文 —— 因此下一个词很显然就应该是 <code>sky</code>。在这样的场景中，相关的信息和预测的词位置之间的间隔是非常小的，<code>RNN</code> 可以学会使用先前的信息。</p>
<p><img src="http://ogtxggxo6.bkt.clouddn.com/rnn4.png?imageView2/2/w/650" alt="不太长的相关信息和位置间隔"></p>
<p>但是同样会有一些更加复杂的场景。假设我们试着去预测<code>“I grew up in France... I speak fluent French”</code>最后的词。当前的信息建议下一个词可能是一种语言的名字，但是如果我们需要弄清楚是什么语言，我们是需要先前提到的离当前位置很远的 <code>France</code>的上下文的。这说明相关信息和当前预测位置之间的间隔就肯定变得相当的大。<br>不幸的是，在这个间隔不断增大时，<code>RNN</code>会丧失学习到连接如此远的信息的能力。</p>
<p><img src="http://ogtxggxo6.bkt.clouddn.com/rnn5.png?imageView2/2/w/650" alt="相当长的相关信息和位置间隔"></p>
<p>在理论上，RNN 绝对可以处理这样的 长期依赖 问题。人们可以仔细挑选参数来解决这类问题中的最初级形式，但在实践中，RNN 肯定不能够成功学习到这些知识。<a href="http://www-dsi.ing.unifi.it/~paolo/ps/tnn-94-gradient.pdf" target="_blank" rel="external">Bengio, et al. (1994)</a>等人对该问题进行了深入的研究，他们发现一些使训练 RNN 变得非常困难的相当根本的原因。<br>然而，幸运的是，LSTM 并没有这个问题！</p>
<h2 id="3、LSTM-网络"><a href="#3、LSTM-网络" class="headerlink" title="3、LSTM 网络"></a>3、LSTM 网络</h2><p><code>Long Short Term</code>网络—— 一般就叫做 <code>LSTM</code> ——是一种 <code>RNN</code> 特殊的类型，可以学习长期依赖信息。LSTM 由<a href="http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf" target="_blank" rel="external">Hochreiter &amp; Schmidhuber (1997)</a>提出，并在近期被<a href="https://scholar.google.com/citations?user=DaFHynwAAAAJ&amp;hl=en" target="_blank" rel="external">Alex Graves</a>进行了改良和推广。在很多问题，<code>LSTM</code> 都取得相当巨大的成功，并得到了广泛的使用。<br><code>LSTM</code>通过刻意的设计来避免长期依赖问题。记住长期的信息在实践中是 <code>LSTM</code> 的默认行为，而非需要付出很大代价才能获得的能力！<br>所有 <code>RNN</code> 都具有一种重复神经网络模块的链式的形式。在标准的 <code>RNN</code> 中，这个重复的模块只有一个非常简单的结构，例如一个 <code>tanh</code> 层。</p>
<p><img src="http://ogtxggxo6.bkt.clouddn.com/rnn6.png?imageView2/2/w/650" alt="标准 RNN 中的重复模块包含单一的层"></p>
<p><code>LSTM</code> 同样是这样的结构，但是重复的模块拥有一个不同的结构。不同于 单一神经网络层，这里是有四个，以一种非常特殊的方式进行交互。</p>
<p><img src="http://ogtxggxo6.bkt.clouddn.com/rnn7.png?imageView2/2/w/650" alt="LSTM 中的重复模块包含四个交互的层"></p>
<p>不必担心这里的细节。我们会一步一步地剖析 LSTM 解析图。现在，我们先来熟悉一下图中使用的各种元素的图标。</p>
<p><img src="http://ogtxggxo6.bkt.clouddn.com/rnn8.png?imageView2/2/w/650" alt="LSTM 中的图标"></p>
<p>在上面的图例中，每一条黑线传输着一整个向量，从一个节点的输出到其他节点的输入。粉色的圈代表<code>pointwise</code>的操作，诸如向量的和，而黄色的矩阵就是学习到的神经网络层。合在一起的线表示向量的连接，分开的线表示内容被复制，然后分发到不同的位置。</p>
<h2 id="4、LSTM-的核心思想"><a href="#4、LSTM-的核心思想" class="headerlink" title="4、LSTM 的核心思想"></a>4、LSTM 的核心思想</h2><p><code>LSTMs</code>的关键点是单元状态，就是穿过图中的水平线。单元状态有点像是个传送带。它贯穿整个链条，只有一些小的线性相互作用。这很容易让信息以不变的方式向下流动。</p>
<p><img src="http://ogtxggxo6.bkt.clouddn.com/rnn9.png?imageView2/2/w/650" alt=""></p>
<p><code>LSTM</code> 有通过精心设计的称作为“门”的结构来去除或者增加信息到细胞状态的能力。门是一种让信息选择式通过的方法。他们包含一个 <code>sigmoid</code> 神经网络层和一个 <code>pointwise</code> 逐点乘法操作。</p>
<p><img src="http://ogtxggxo6.bkt.clouddn.com/rnn10.png?imageView2/2/w/650" alt=""></p>
<p><code>Sigmoid</code>层输出 0 到 1 之间的数值，描述每个部分有多少量可以通过。0 代表“不许任何量通过”，1 就指“允许任意量通过”！</p>
<p><code>LSTM</code>拥有三个门，来保护和控制细胞状态。</p>
<h2 id="4、逐步理解-LSTM"><a href="#4、逐步理解-LSTM" class="headerlink" title="4、逐步理解 LSTM"></a>4、逐步理解 LSTM</h2><p>在我们<code>LSTM</code>中的第一步是决定我们会从细胞状态中丢弃什么信息。这个决定通过一个称为<strong>遗忘门</strong>完成。该门会读取 $h_{t-1}$ 和 $x<em>t$，输出一个在 0 到 1 之间的数值给每个在细胞状态 $C</em>{t-1}$ 中的数字。1 表示“完全保留”，0 表示“完全舍弃”。<br>让我们回到语言模型的例子中来基于已经看到的预测下一个词。在这个问题中，细胞状态可能包含当前<strong>主语</strong>的性别，因此正确的<strong>代词</strong>可以被选择出来。当我们看到新的<strong>主语</strong>，我们希望忘记旧的<strong>主语</strong>。</p>
<p><img src="http://ogtxggxo6.bkt.clouddn.com/rnn11.png?imageView2/2/w/650" alt=""></p>
<p>下一步是确定什么样的新信息被存放在细胞状态中。这里包含两个部分。第一，<code>sigmoid</code>层称 “<strong>输入门</strong>” 决定什么值我们将要更新。然后，一个 <code>tanh</code> 层创建一个新的候选值向量$\tilde{C}_t$，会被加入到状态中。下一步，我们会讲这两个信息来产生对状态的更新。<br>在我们语言模型的例子中，我们希望增加新的主语的性别到细胞状态中，来替代旧的需要忘记的主语。</p>
<p><img src="http://ogtxggxo6.bkt.clouddn.com/rnn12.png?imageView2/2/w/650" alt=""></p>
<p>现在是更新旧细胞状态的时间了，$C_{t-1}$ 更新为 $C_t$。前面的步骤已经决定了将会做什么，我们现在就是实际去完成。<br>我们把旧状态与 $f_t$ 相乘，丢弃掉我们确定需要丢弃的信息。接着加上 $i_t * \tilde{C}_t$。这就是新的候选值，根据我们决定更新每个状态的程度进行变化。<br>在语言模型的例子中，这就是我们实际根据前面确定的目标，丢弃旧代词的性别信息并添加新的信息的地方。</p>
<p><img src="http://ogtxggxo6.bkt.clouddn.com/rnn13.png?imageView2/2/w/650" alt=""></p>
<p>最终，我们需要确定输出什么值，即“<strong>输出门</strong>”。这个输出将会基于我们的细胞状态，但是也是一个过滤后的版本。首先，我们运行一个<code>sigmoid</code> 层来确定细胞状态的哪个部分将输出出去。接着，我们把细胞状态通过<code>tanh</code>进行处理（得到一个在 -1 到 1 之间的值）并将它和 <code>sigmoid</code> 门的输出相乘，最终我们仅仅会输出我们确定输出的那部分。<br>在语言模型的例子中，因为他就看到了一个 <strong>代词</strong>，可能需要输出与一个 <strong>动词</strong> 相关的信息。例如，可能输出是否代词是单数还是负数，这样如果是动词的话，我们也知道动词需要进行的词形变化。</p>
<p><img src="http://ogtxggxo6.bkt.clouddn.com/rnn14.png?imageView2/2/w/650" alt=""></p>
<h2 id="5、LSTM-的变体"><a href="#5、LSTM-的变体" class="headerlink" title="5、LSTM 的变体"></a>5、LSTM 的变体</h2><p>我们到目前为止都还在介绍正常的 LSTM。但是不是所有的 LSTM 都长成一个样子的。实际上，几乎所有包含 LSTM 的论文都采用了微小的变体。差异非常小，但是也值得拿出来讲一下。<br>其中一个流形的 LSTM 变体，就是由 <a href="ftp://ftp.idsia.ch/pub/juergen/TimeCount-IJCNN2000.pdf" target="_blank" rel="external">Gers &amp; Schmidhuber (2000)</a> 提出的，增加了 “peephole connection”。是说，我们让 门层 也会接受细胞状态的输入。</p>
<p><img src="http://ogtxggxo6.bkt.clouddn.com/rnn15.png?imageView2/2/w/650" alt=""></p>
<p>上面的图例中，我们增加了<code>peephole</code> 到每个门上，但是许多论文会加入部分的 <code>peephole</code> 而非所有都加。</p>
<p>另一个变体是通过使用 <code>coupled</code>耦合忘记和输入门。不同于之前是分开确定什么忘记和需要添加什么新的信息，这里是一同做出决定。我们仅仅会当我们将要输入在当前位置时忘记。我们仅仅输入新的值到那些我们已经忘记旧的信息的那些状态 。</p>
<p><img src="http://ogtxggxo6.bkt.clouddn.com/rnn16.png?imageView2/2/w/650" alt=""></p>
<p>另一个改动较大的变体是 Gated Recurrent Unit (GRU)，这是由 <a href="http://arxiv.org/pdf/1406.1078v3.pdf" target="_blank" rel="external">Cho, et al. (2014)</a> 提出。它将忘记门和输入门合成了一个单一的 更新门。同样还混合了细胞状态和隐藏状态，和其他一些改动。最终的模型比标准的 LSTM 模型要简单，也是非常流行的变体。</p>
<p><img src="http://ogtxggxo6.bkt.clouddn.com/rnn17.png?imageView2/2/w/650" alt=""></p>
<p>这里只是部分流行的 <code>LSTM</code> 变体。当然还有很多其他的，如<a href="http://arxiv.org/pdf/1508.03790v2.pdf" target="_blank" rel="external">Yao, et al. (2015)</a> 提出的 <code>Depth Gated RNN</code>。还有用一些完全不同的观点来解决长期依赖的问题，如<a href="http://arxiv.org/pdf/1402.3511v1.pdf" target="_blank" rel="external">Koutnik, et al. (2014)</a> 提出的 <code>Clockwork RNN</code>。<br>要问哪个变体是最好的？其中的差异性真的重要吗？<a href="http://arxiv.org/pdf/1503.04069.pdf" target="_blank" rel="external">Greff, et al. (2015)</a> 给出了流行变体的比较，结论是他们基本上是一样的。<a href="http://jmlr.org/proceedings/papers/v37/jozefowicz15.pdf" target="_blank" rel="external">Jozefowicz, et al. (2015)</a> 则在超过 1 万种 <code>RNN</code> 架构上进行了测试，发现一些架构在某些任务上也取得了比<code>LSTM</code>更好的结果。</p>
<p><img src="http://ogtxggxo6.bkt.clouddn.com/rnn18.png?imageView2/2/w/650" alt=""></p>
<h2 id="八、结论"><a href="#八、结论" class="headerlink" title="八、结论"></a>八、结论</h2><p>刚开始，我提到通过 <code>RNN</code> 得到重要的结果。本质上所有这些都可以使用 <code>LSTM</code> 完成。对于大多数任务确实展示了更好的性能！<br>由于<code>LSTM</code>一般是通过一系列的方程表示的，使得 <code>LSTM</code>有一点令人费解。然而本文中一步一步地解释让这种困惑消除了不少。<br><code>LSTM</code>是我们在<code>RNN</code>中获得的重要成功。很自然地，我们也会考虑：哪里会有更加重大的突破呢？在研究人员间普遍的观点是：“Yes! 下一步已经有了——那就是<strong>注意力</strong>！” 这个想法是让 <code>RNN</code> 的每一步都从更加大的信息集中挑选信息。例如，如果你使用 RNN 来产生一个图片的描述，可能会选择图片的一个部分，根据这部分信息来产生输出的词。实际上，<a href="http://arxiv.org/pdf/1502.03044v2.pdf" target="_blank" rel="external">Xu, <em>et al.</em>(2015)</a>已经这么做了——如果你希望深入探索<strong>注意力</strong>可能这就是一个有趣的起点！还有一些使用注意力的相当振奋人心的研究成果，看起来有更多的东西亟待探索……<br>注意力也不是 RNN 研究领域中唯一的发展方向。例如，<a href="http://arxiv.org/pdf/1507.01526v1.pdf" target="_blank" rel="external">Kalchbrenner, <em>et al.</em> (2015)</a> 提出的 <code>Grid LSTM</code>看起来也是很有前途。使用生成模型的 <code>RNN</code>，诸如<a href="http://arxiv.org/pdf/1502.04623.pdf" target="_blank" rel="external">Gregor, <em>et al.</em> (2015)</a> <a href="http://arxiv.org/pdf/1506.02216v3.pdf" target="_blank" rel="external">Chung, <em>et al.</em> (2015)</a> 和 <a href="http://arxiv.org/pdf/1411.7610v3.pdf" target="_blank" rel="external">Bayer &amp; Osendorfer (2015)</a> 提出的模型同样很有趣。在过去几年中，<code>RNN</code> 的研究已经相当的燃，而研究成果当然也会更加丰富！</p>
<h2 id="七、致谢"><a href="#七、致谢" class="headerlink" title="七、致谢"></a>七、致谢</h2><p>I’m grateful to a number of people for helping me better understand LSTMs, commenting on the visualizations, and providing feedback on this post.<br>I’m very grateful to my colleagues at Google for their helpful feedback, especially <a href="http://research.google.com/pubs/OriolVinyals.html" target="_blank" rel="external">Oriol Vinyals</a>,<a href="http://research.google.com/pubs/GregCorrado.html" target="_blank" rel="external">Greg Corrado</a>, <a href="http://research.google.com/pubs/JonathonShlens.html" target="_blank" rel="external">Jon Shlens</a>, <a href="http://people.cs.umass.edu/~luke/" target="_blank" rel="external">Luke Vilnis</a>, and <a href="http://www.cs.toronto.edu/~ilya/" target="_blank" rel="external">Ilya Sutskever</a>. I’m also thankful to many other friends and colleagues for taking the time to help me, including <a href="https://www.linkedin.com/pub/dario-amodei/4/493/393" target="_blank" rel="external">Dario Amodei</a>, and <a href="http://cs.stanford.edu/~jsteinhardt/" target="_blank" rel="external">Jacob Steinhardt</a>. I’m especially thankful to <a href="http://www.kyunghyuncho.me/" target="_blank" rel="external">Kyunghyun Cho</a> for extremely thoughtful correspondence about my diagrams.<br>Before this post, I practiced explaining LSTMs during two seminar series I taught on neural networks. Thanks to everyone who participated in those for their patience with me, and for their feedback.</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://petersansan.top/2017/04/29/转-理解LSTM网络/" data-id="cj708zkh3003dgaif5dls3260" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/lstm/">lstm</a></li></ul>

    </footer>
	
    <div id="cloud-tie-wrapper" class="cloud-tie-wrapper"></div>
    <script src="https://img1.cache.netease.com/f2e/tie/yun/sdk/loader.js"></script>
    <script>
    var cloudTieConfig = {
      url: document.location.href, 
      sourceId: "",
      productKey: "e2a8be6e9b7f4bf7aa3c3f956be2ce37",
      target: "cloud-tie-wrapper"
    };
    var yunManualLoad = true;
    Tie.loader("aHR0cHM6Ly9hcGkuZ2VudGllLjE2My5jb20vcGMvbGl2ZXNjcmlwdC5odG1s", true);
    </script>
	
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2017/04/28/论文记录0/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          RECURRENT NEURAL NETWORK REGULARIZATION
        
      </div>
    </a>
  
  
    <a href="/2017/04/29/采样采样/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">采样采样</div>
    </a>
  
</nav>

  
</article>


</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">分类</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Keras/">Keras</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/NLP/">NLP</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/tensorflow/">tensorflow</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/工具/">工具</a><span class="category-list-count">6</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/序列标注/">序列标注</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/服务器/">服务器</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/机器学习/">机器学习</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/深度学习/">深度学习</a><span class="category-list-count">6</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/理论/">理论</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/算法/">算法</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/论文记录/">论文记录</a><span class="category-list-count">6</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/读后感/">读后感</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/资源/">资源</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/随笔/">随笔</a><span class="category-list-count">3</span></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">标签云</h3>
    <div class="widget tagcloud">
      <a href="/tags/Keras/" style="font-size: 10px;">Keras</a> <a href="/tags/LSTM/" style="font-size: 13.33px;">LSTM</a> <a href="/tags/NLP/" style="font-size: 16.67px;">NLP</a> <a href="/tags/github/" style="font-size: 13.33px;">github</a> <a href="/tags/hexo/" style="font-size: 10px;">hexo</a> <a href="/tags/jupyter/" style="font-size: 13.33px;">jupyter</a> <a href="/tags/lstm/" style="font-size: 10px;">lstm</a> <a href="/tags/nlp/" style="font-size: 10px;">nlp</a> <a href="/tags/tensorflow/" style="font-size: 10px;">tensorflow</a> <a href="/tags/ubuntu/" style="font-size: 10px;">ubuntu</a> <a href="/tags/webapp/" style="font-size: 10px;">webapp</a> <a href="/tags/中文分词/" style="font-size: 10px;">中文分词</a> <a href="/tags/价值观/" style="font-size: 10px;">价值观</a> <a href="/tags/工具/" style="font-size: 13.33px;">工具</a> <a href="/tags/序列标注/" style="font-size: 20px;">序列标注</a> <a href="/tags/服务器/" style="font-size: 10px;">服务器</a> <a href="/tags/机器学习/" style="font-size: 13.33px;">机器学习</a> <a href="/tags/概率图模型/" style="font-size: 10px;">概率图模型</a> <a href="/tags/深度学习/" style="font-size: 20px;">深度学习</a> <a href="/tags/理论/" style="font-size: 10px;">理论</a> <a href="/tags/算法/" style="font-size: 13.33px;">算法</a> <a href="/tags/综述/" style="font-size: 10px;">综述</a> <a href="/tags/读后感/" style="font-size: 16.67px;">读后感</a> <a href="/tags/转载/" style="font-size: 10px;">转载</a> <a href="/tags/递归/" style="font-size: 10px;">递归</a> <a href="/tags/随笔/" style="font-size: 10px;">随笔</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">标签</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Keras/">Keras</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/LSTM/">LSTM</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/NLP/">NLP</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/github/">github</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/hexo/">hexo</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/jupyter/">jupyter</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/lstm/">lstm</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/nlp/">nlp</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/tensorflow/">tensorflow</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ubuntu/">ubuntu</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/webapp/">webapp</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/中文分词/">中文分词</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/价值观/">价值观</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/工具/">工具</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/序列标注/">序列标注</a><span class="tag-list-count">7</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/服务器/">服务器</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/机器学习/">机器学习</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/概率图模型/">概率图模型</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/深度学习/">深度学习</a><span class="tag-list-count">7</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/理论/">理论</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/算法/">算法</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/综述/">综述</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/读后感/">读后感</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/转载/">转载</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/递归/">递归</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/随笔/">随笔</a><span class="tag-list-count">1</span></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">归档</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/08/">八月 2017</a><span class="archive-list-count">8</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/05/">五月 2017</a><span class="archive-list-count">11</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/04/">四月 2017</a><span class="archive-list-count">20</span></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">最新文章</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2017/08/31/论文记录5/">A Long Dependency Aware Deep Architecture for Joint Chinese Word Segmentation and POS Tagging</a>
          </li>
        
          <li>
            <a href="/2017/08/31/论文记录4/">Attending to Characters in Neural Sequence Labeling Models</a>
          </li>
        
          <li>
            <a href="/2017/08/31/论文记录3/">End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF</a>
          </li>
        
          <li>
            <a href="/2017/08/27/哪有什么岁月静好，不过是有人替你负重前行/">哪有什么岁月静好，不过是有人替你负重前行</a>
          </li>
        
          <li>
            <a href="/2017/08/27/seq2seq/">seq2seq模型分析</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2017 PeterSansan<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
    <a href="/about" class="mobile-nav-link">About</a>
  
    <a href="/other" class="mobile-nav-link">TensorFlow</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>

  </div><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
</body>
</html>